{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d87a62-732a-45f4-bdee-c10ca878f7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. What is the mathematical formula for a linear SVM?\n",
    "\n",
    "The mathematical formula for a linear Support Vector Machine (SVM) can be expressed as follows:\n",
    "\n",
    "Given a training dataset consisting of \\(n\\) samples with \\(m\\) features, represented as \\((\\mathbf{x}_1, y_1), (\\mathbf{x}_2, y_2), \\ldots, (\\mathbf{x}_n, y_n)\\), where \\(\\mathbf{x}_i\\) is the feature vector of the \\(i\\)th sample, and \\(y_i\\) is its corresponding class label (-1 for the negative class and +1 for the positive class), the linear SVM aims to find the optimal hyperplane that separates the two classes with the maximum margin.\n",
    "\n",
    "The decision function of a linear SVM is defined as:\n",
    "\n",
    "\\[ f(\\mathbf{x}) = \\mathbf{w} \\cdot \\mathbf{x} + b \\]\n",
    "\n",
    "Where:\n",
    "- \\(\\mathbf{x}\\) is the input feature vector.\n",
    "- \\(\\mathbf{w}\\) is the weight vector perpendicular to the hyperplane.\n",
    "- \\(b\\) is the bias term.\n",
    "\n",
    "The classification rule is then determined by the sign of the decision function:\n",
    "\n",
    "\\[ \\hat{y} = \\text{sign}(f(\\mathbf{x})) \\]\n",
    "\n",
    "The optimization problem associated with linear SVM can be formulated as:\n",
    "\n",
    "\\[ \\min_{\\mathbf{w}, b} \\frac{1}{2} \\| \\mathbf{w} \\|^2 \\]\n",
    "\n",
    "Subject to the constraints:\n",
    "\n",
    "\\[ y_i(\\mathbf{w} \\cdot \\mathbf{x}_i + b) \\geq 1, \\text{ for } i = 1, 2, \\ldots, n \\]\n",
    "\n",
    "This is the formulation for the hard-margin linear SVM. In practice, soft-margin SVM is often used, which allows for some misclassification by introducing slack variables and adjusting the cost parameter \\(C\\).\n",
    "\n",
    "The optimization problem for soft-margin SVM becomes:\n",
    "\n",
    "\\[ \\min_{\\mathbf{w}, b} \\frac{1}{2} \\| \\mathbf{w} \\|^2 + C \\sum_{i=1}^{n} \\xi_i \\]\n",
    "\n",
    "Subject to the constraints:\n",
    "\n",
    "\\[ y_i(\\mathbf{w} \\cdot \\mathbf{x}_i + b) \\geq 1 - \\xi_i, \\text{ for } i = 1, 2, \\ldots, n \\]\n",
    "\\[ \\xi_i \\geq 0, \\text{ for } i = 1, 2, \\ldots, n \\]\n",
    "\n",
    "Here, \\(C\\) is the regularization parameter that controls the trade-off between maximizing the margin and minimizing the classification error. Larger values of \\(C\\) lead to a narrower margin but fewer misclassifications, while smaller values of \\(C\\) result in a wider margin but potentially more misclassifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0391575-d00e-4872-b9eb-8f3785bc4cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2. What is the objective function of a linear SVM?\n",
    "\n",
    "The objective function of a linear Support Vector Machine (SVM) is a mathematical expression that the SVM aims to minimize during the training process. The objective function is typically formulated to find the hyperplane that maximizes the margin between the classes while minimizing the classification error.\n",
    "\n",
    "For a linear SVM, the objective function is defined as:\n",
    "\n",
    "\\[ \\min_{\\mathbf{w}, b} \\frac{1}{2} \\| \\mathbf{w} \\|^2 \\]\n",
    "\n",
    "This objective function aims to find the weight vector \\(\\mathbf{w}\\) and bias term \\(b\\) that minimize the norm of the weight vector, subject to certain constraints. The norm of the weight vector represents the margin, and minimizing it leads to maximizing the margin between the classes.\n",
    "\n",
    "The constraints imposed on the optimization problem ensure that the training samples are correctly classified or lie within a certain margin from the decision boundary. These constraints are typically expressed as:\n",
    "\n",
    "\\[ y_i(\\mathbf{w} \\cdot \\mathbf{x}_i + b) \\geq 1 \\]\n",
    "\n",
    "where \\(y_i\\) is the class label of the \\(i\\)th training sample, \\(\\mathbf{x}_i\\) is its feature vector, and \\(\\mathbf{w} \\cdot \\mathbf{x}_i + b\\) represents the decision function. This constraint ensures that each sample is on the correct side of the decision boundary and lies outside a certain margin from it.\n",
    "\n",
    "In the case of soft-margin SVM, which allows for some misclassification, the objective function is modified to include a term that penalizes misclassifications:\n",
    "\n",
    "\\[ \\min_{\\mathbf{w}, b} \\frac{1}{2} \\| \\mathbf{w} \\|^2 + C \\sum_{i=1}^{n} \\xi_i \\]\n",
    "\n",
    "where \\(C\\) is the regularization parameter that controls the trade-off between maximizing the margin and minimizing the classification error, and \\(\\xi_i\\) are slack variables that represent the extent of misclassification for each sample.\n",
    "\n",
    "Overall, the objective function of a linear SVM aims to find the hyperplane that maximizes the margin between the classes while ensuring that the training samples are correctly classified or lie within a certain margin from the decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f228f16-d071-45e3-8c10-163dbbb645b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3. What is the kernel trick in SVM?\n",
    "\n",
    "The kernel trick is a technique used in Support Vector Machines (SVMs) to implicitly map input data into a higher-dimensional feature space without explicitly computing the transformed feature vectors. It allows SVMs to efficiently handle non-linearly separable data by transforming it into a higher-dimensional space where it may become linearly separable.\n",
    "\n",
    "The kernel trick works by introducing a kernel function \\( K(\\mathbf{x}_i, \\mathbf{x}_j) \\), which computes the inner product (dot product) of the input feature vectors \\(\\mathbf{x}_i\\) and \\(\\mathbf{x}_j\\) in the transformed feature space. Instead of explicitly transforming the input data into the higher-dimensional space, the kernel function calculates the similarity between pairs of data points directly in the original feature space.\n",
    "\n",
    "Mathematically, the decision function of an SVM using the kernel trick can be expressed as:\n",
    "\n",
    "\\[ f(\\mathbf{x}) = \\sum_{i=1}^{n} \\alpha_i y_i K(\\mathbf{x}_i, \\mathbf{x}) + b \\]\n",
    "\n",
    "Where:\n",
    "- \\( \\alpha_i \\) are the Lagrange multipliers obtained during the optimization process.\n",
    "- \\( y_i \\) are the class labels of the training samples.\n",
    "- \\( \\mathbf{x}_i \\) are the training feature vectors.\n",
    "- \\( \\mathbf{x} \\) is the input feature vector.\n",
    "- \\( K(\\mathbf{x}_i, \\mathbf{x}) \\) is the kernel function, which computes the inner product (similarity) between \\( \\mathbf{x}_i \\) and \\( \\mathbf{x} \\) in the original feature space.\n",
    "\n",
    "Commonly used kernel functions include:\n",
    "\n",
    "1. Linear kernel: \\( K(\\mathbf{x}_i, \\mathbf{x}_j) = \\mathbf{x}_i^T \\mathbf{x}_j \\)\n",
    "2. Polynomial kernel: \\( K(\\mathbf{x}_i, \\mathbf{x}_j) = (\\gamma \\mathbf{x}_i^T \\mathbf{x}_j + r)^d \\)\n",
    "3. Gaussian (RBF) kernel: \\( K(\\mathbf{x}_i, \\mathbf{x}_j) = \\exp \\left( - \\frac{\\| \\mathbf{x}_i - \\mathbf{x}_j \\|^2}{2\\sigma^2} \\right) \\)\n",
    "\n",
    "Using the kernel trick, SVMs can effectively learn complex decision boundaries in the original feature space without explicitly computing the transformed feature vectors, making them powerful and versatile classifiers for handling non-linearly separable data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af72056f-6b3f-46ba-94d1-f626374d6dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4. What is the role of support vectors in SVM Explain with example\n",
    "\n",
    "In Support Vector Machines (SVMs), support vectors are the data points that lie closest to the decision boundary (hyperplane) and influence the position and orientation of the hyperplane. These points are crucial in determining the optimal hyperplane that maximizes the margin between the classes.\n",
    "\n",
    "The role of support vectors in SVM can be understood through the following key points:\n",
    "\n",
    "1. **Defining the Margin**: In SVM, the margin is the distance between the decision boundary and the closest data points from each class. The support vectors are the data points that lie on the margin or within a certain distance from it. They define the margin and are essential for maximizing it.\n",
    "\n",
    "2. **Determining the Decision Boundary**: The decision boundary of an SVM is determined by the support vectors. The hyperplane is positioned such that it is equidistant from the closest support vectors of each class. This ensures that the decision boundary is optimally positioned to separate the classes while maximizing the margin.\n",
    "\n",
    "3. **Influence on Model's Parameters**: During the training process, the optimization algorithm of SVM focuses on the support vectors, as they have the most significant impact on the position and orientation of the decision boundary. The model parameters, such as the weight vector and bias term, are computed based on the support vectors.\n",
    "\n",
    "4. **Robustness and Generalization**: Support vectors represent the most informative data points in the dataset that are critical for the classification task. As a result, the decision boundary of an SVM is robust and less sensitive to outliers or noise in the data. This leads to better generalization performance on unseen data.\n",
    "\n",
    "Example:\n",
    "Consider a binary classification problem where we aim to classify points in a two-dimensional feature space into two classes, 'A' and 'B', using an SVM. Let's assume the following points are our training data:\n",
    "\n",
    "- Class 'A': (-1, -1), (0, 0)\n",
    "- Class 'B': (1, 1), (2, 2)\n",
    "\n",
    "In this case, the support vectors would be the points closest to the decision boundary. Here, the support vectors would be (-1, -1) and (1, 1), as they lie on the margin or within a certain distance from it. These support vectors determine the position and orientation of the decision boundary, which optimally separates the two classes. Other data points that are farther away from the decision boundary do not influence the hyperplane and are not considered support vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c99b38-b854-479f-b203-68a2bd6fe8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5. Illustrate with examples and graphs of Hyperplane, Marginal plane, Soft margin and Hard margin in\n",
    "SVM?\n",
    "\n",
    "Certainly! Let's illustrate the concepts of hyperplane, marginal plane, soft margin, and hard margin in SVM with examples and graphs.\n",
    "\n",
    "### Example 1: Linearly Separable Data (Hard Margin)\n",
    "\n",
    "Consider a simple example with two classes, 'A' and 'B', that are linearly separable.\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Generate linearly separable data\n",
    "X, y = make_classification(n_samples=100, n_features=2, n_classes=2, n_clusters_per_class=1, random_state=42)\n",
    "\n",
    "# Fit SVM with hard margin\n",
    "svm_hard = SVC(kernel='linear', C=1e6)\n",
    "svm_hard.fit(X, y)\n",
    "\n",
    "# Plot data points and decision boundary\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis', marker='o', edgecolors='k', label='Data Points')\n",
    "\n",
    "# Plot decision boundary\n",
    "ax = plt.gca()\n",
    "xlim = ax.get_xlim()\n",
    "ylim = ax.get_ylim()\n",
    "\n",
    "# Create grid to evaluate model\n",
    "xx = np.linspace(xlim[0], xlim[1], 30)\n",
    "yy = np.linspace(ylim[0], ylim[1], 30)\n",
    "YY, XX = np.meshgrid(yy, xx)\n",
    "xy = np.vstack([XX.ravel(), YY.ravel()]).T\n",
    "Z = svm_hard.decision_function(xy).reshape(XX.shape)\n",
    "\n",
    "# Plot decision boundary and margins\n",
    "ax.contour(XX, YY, Z, colors='k', levels=[-1, 0, 1], alpha=0.5, linestyles=['--', '-', '--'])\n",
    "ax.scatter(svm_hard.support_vectors_[:, 0], svm_hard.support_vectors_[:, 1], s=100, linewidth=1, facecolors='none',\n",
    "           edgecolors='k', label='Support Vectors')\n",
    "\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('SVM with Hard Margin')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "In this example, we use a linear SVM with a hard margin (large \\(C\\)) to separate the two classes. The decision boundary (hyperplane) is shown as a solid line, and the margin is represented by the dashed lines. The support vectors, which lie on the margin or within a certain distance from it, are denoted by black circles.\n",
    "\n",
    "### Example 2: Linearly Inseparable Data (Soft Margin)\n",
    "\n",
    "Now, let's consider an example with data that is not linearly separable.\n",
    "\n",
    "# Generate linearly inseparable data\n",
    "X, y = make_classification(n_samples=100, n_features=2, n_classes=2, n_clusters_per_class=1,\n",
    "                           flip_y=0.1, class_sep=0.5, random_state=42)\n",
    "\n",
    "# Fit SVM with soft margin\n",
    "svm_soft = SVC(kernel='linear', C=0.1)\n",
    "svm_soft.fit(X, y)\n",
    "\n",
    "# Plot data points and decision boundary\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis', marker='o', edgecolors='k', label='Data Points')\n",
    "\n",
    "# Plot decision boundary\n",
    "ax = plt.gca()\n",
    "xlim = ax.get_xlim()\n",
    "ylim = ax.get_ylim()\n",
    "\n",
    "# Create grid to evaluate model\n",
    "xx = np.linspace(xlim[0], xlim[1], 30)\n",
    "yy = np.linspace(ylim[0], ylim[1], 30)\n",
    "YY, XX = np.meshgrid(yy, xx)\n",
    "xy = np.vstack([XX.ravel(), YY.ravel()]).T\n",
    "Z = svm_soft.decision_function(xy).reshape(XX.shape)\n",
    "\n",
    "# Plot decision boundary and margins\n",
    "ax.contour(XX, YY, Z, colors='k', levels=[-1, 0, 1], alpha=0.5, linestyles=['--', '-', '--'])\n",
    "ax.scatter(svm_soft.support_vectors_[:, 0], svm_soft.support_vectors_[:, 1], s=100, linewidth=1, facecolors='none',\n",
    "           edgecolors='k', label='Support Vectors')\n",
    "\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('SVM with Soft Margin')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "In this example, we use a linear SVM with a soft margin (small \\(C\\)) to handle the linearly inseparable data. The decision boundary (hyperplane) adjusts to accommodate misclassified points within the margin, represented by the dashed lines.\n",
    "\n",
    "These examples illustrate the concepts of hard margin and soft margin in SVM and how the SVM adapts to different scenarios based on the margin parameter \\(C\\)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8bc8d30-47c1-4ce6-855b-49e5566fe5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q6. SVM Implementation through Iris dataset.\n",
    "\n",
    "~ Load the iris dataset from the scikit-learn library and split it into a training set and a testing setl\n",
    "~ Train a linear SVM classifier on the training set and predict the labels for the testing setl\n",
    "~ Compute the accuracy of the model on the testing setl\n",
    "~ Plot the decision boundaries of the trained model using two of the featuresl\n",
    "~ Try different values of the regularisation parameter C and see how it affects the performance of\n",
    "the model.\n",
    "\n",
    "Bonus task: Implement a linear SVM classifier from scratch using Python and compare its\n",
    "performance with the scikit-learn implementation.\n",
    "\n",
    "\n",
    "Here's how you can implement SVM on the Iris dataset using scikit-learn:\n",
    "\n",
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, :2]  # Only take the first two features for visualization\n",
    "y = iris.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train a linear SVM classifier\n",
    "svm_classifier = SVC(kernel='linear', C=1.0)\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels for the testing set\n",
    "y_pred = svm_classifier.predict(X_test)\n",
    "\n",
    "# Compute the accuracy of the model on the testing set\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Plot the decision boundaries of the trained model\n",
    "def plot_decision_boundary(X, y, classifier, title):\n",
    "    h = .02  # Step size in the mesh\n",
    "    cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])\n",
    "    cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])\n",
    "\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "\n",
    "    Z = classifier.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.figure()\n",
    "    plt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold, edgecolor='k', s=20)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Sepal length')\n",
    "    plt.ylabel('Sepal width')\n",
    "    plt.show()\n",
    "\n",
    "plot_decision_boundary(X_train, y_train, svm_classifier, title=\"Decision Boundary of Linear SVM (Training Set)\")\n",
    "\n",
    "# Try different values of the regularization parameter C\n",
    "for C in [0.1, 1, 10]:\n",
    "    svm_classifier = SVC(kernel='linear', C=C)\n",
    "    svm_classifier.fit(X_train, y_train)\n",
    "    y_pred = svm_classifier.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(\"Accuracy (C={}):\".format(C), accuracy)\n",
    "    plot_decision_boundary(X_train, y_train, svm_classifier, title=\"Decision Boundary of Linear SVM (C={})\".format(C))\n",
    "\n",
    "This code first loads the Iris dataset, splits it into training and testing sets, trains a linear SVM classifier on the training set, and predicts the labels for the testing set. It then computes the accuracy of the model on the testing set and plots the decision boundaries of the trained model using the first two features. Finally, it tries different values of the regularization parameter \\( C \\) and observes how it affects the performance of the model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
